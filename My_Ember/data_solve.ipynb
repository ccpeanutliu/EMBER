{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jsonlines\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡負責label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keys = ['appeared', 'exports',  'label', 'histogram', 'imports', 'general', 'sha256', 'section', 'byteentropy', 'header']\n",
    "keys_cnn = ['strings','sha256', 'appeared', 'histogram', 'byteentropy', 'general']#[9,1,1,256,256,10]\n",
    "keys_rnn = ['exports', 'imports', 'section', 'header']\n",
    "\n",
    "keys_strings = ['avlength', 'numstrings', 'registry', 'urls', 'MZ', 'printables', 'entropy', 'paths', 'printabledist']\n",
    "keys_general = ['exports', 'has_resources', 'imports', 'symbols', 'has_signature', 'has_relocations', 'has_debug', 'vsize', 'size', 'has_tls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "170000\n",
      "170000\n",
      "170000\n",
      "170000\n",
      "170000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "#save all label\n",
    "\n",
    "for name in range(0,7):\n",
    "    if name < 6:\n",
    "        path = \"./ember/train_features_\"+str(name)+\".jsonl\"\n",
    "    else:\n",
    "        path = \"./ember/test_features.jsonl\"\n",
    "    origin_feature = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            origin_feature.append(obj)\n",
    "    n = len(origin_feature)\n",
    "    print(n)\n",
    "    keys = ['appeared', 'exports',  'label', 'histogram', 'imports', 'general', 'sha256', 'section', 'byteentropy', 'header']\n",
    "    keys_cnn = ['strings','sha256', 'appeared', 'histogram', 'byteentropy', 'general']#[9,1,1,256,256,10]\n",
    "    keys_rnn = ['exports', 'imports', 'section', 'header']\n",
    "    \n",
    "    keys_strings = ['avlength', 'numstrings', 'registry', 'urls', 'MZ', 'printables', 'entropy', 'paths', 'printabledist']\n",
    "    keys_general = ['exports', 'has_resources', 'imports', 'symbols', 'has_signature', 'has_relocations', 'has_debug', 'vsize', 'size', 'has_tls']\n",
    "    label = []\n",
    "    for i in range(n):\n",
    "        label.append(origin_feature[i]['label'])\n",
    "    np.save(\"./label/\"+ str(name) +\".npy\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡負責字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 07:24:53.665329 140339592378112 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save data...\n",
      "\n",
      "Data 1 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n",
      "Data 2 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n",
      "Data 3 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n",
      "Data 4 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n",
      "Data 5 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n",
      "Data 6 \n",
      "\n",
      "Read Data...\n",
      "\n",
      "Read strings...\n",
      "\n",
      "Start word2vec...\n",
      "\n",
      "Save data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qq = [0,1,2,3,4,5,6]\n",
    "#qq = [6]\n",
    "w2v_model = Word2Vec.load(\"large_w2v.model\")\n",
    "\n",
    "for num in qq:\n",
    "\n",
    "    print(\"Data\",num,\"\\n\")\n",
    "    path = \"./ember/train_features_\"+ str(num) +\".jsonl\"\n",
    "    \n",
    "    if num == 6:\n",
    "        path = \"./ember/test_features.jsonl\"\n",
    "    \n",
    "    print(\"Read Data...\\n\")\n",
    "    origin_feature = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            origin_feature.append(obj)\n",
    "    n = len(origin_feature)\n",
    "    \n",
    "    print(\"Read strings...\\n\")\n",
    "    # imports\n",
    "    \n",
    "    rnn_feature = []\n",
    "    for i in range(n):\n",
    "        rnn_feature.append([])\n",
    "    \n",
    "    for i in range(n):\n",
    "        tmp = origin_feature[i]['imports'].keys()\n",
    "        for key in tmp:\n",
    "            tmp_n = len(origin_feature[i]['imports'][key])\n",
    "            for j in range(tmp_n):\n",
    "                rnn_feature[i].append(origin_feature[i]['imports'][key][j])\n",
    "    '''\n",
    "    # header coff\n",
    "    for i in range(n):\n",
    "        rnn_feature[i].append(origin_feature[i]['header']['coff']['machine'])\n",
    "        tmp_n = len(origin_feature[i]['header']['coff']['characteristics'])\n",
    "        for j in range(tmp_n):\n",
    "            rnn_feature[i].append(origin_feature[i]['header']['coff']['characteristics'][j])\n",
    "    '''\n",
    "\n",
    "    # exports\n",
    "    for i in range(n):\n",
    "        tmp_n = len(origin_feature[i]['exports'])\n",
    "        if tmp_n != 0:\n",
    "            for obj in origin_feature[i]['exports']:\n",
    "                rnn_feature[i].append(obj)\n",
    " \n",
    "    # word2vec\n",
    "   # if num == 1:\n",
    "   #     w2v_model = Word2Vec(rnn_feature,min_count = 4, size=200, sg = 1)\n",
    "   #     w2v_model.save(\"w2v.model\")\n",
    "    print(\"Start word2vec...\\n\")\n",
    "    #embedding\n",
    "    embedding_matrix = np.zeros((len(w2v_model.wv.vocab.items()) + 1, w2v_model.vector_size))\n",
    "    word2idx = {}\n",
    "\n",
    "    vocab_list = [(word, w2v_model.wv[word]) for word, _ in w2v_model.wv.vocab.items()]\n",
    "    for i, vocab in enumerate(vocab_list):\n",
    "        word, vec = vocab\n",
    "        embedding_matrix[i + 1] = vec\n",
    "        word2idx[word] = i + 1\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                                output_dim=embedding_matrix.shape[1],\n",
    "                                mask_zero=True,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=True)\n",
    "    \n",
    "    def text_to_index(corpus):\n",
    "        new_corpus = []\n",
    "        for doc in corpus:\n",
    "            new_doc = []\n",
    "            for word in doc:\n",
    "                try:\n",
    "                    new_doc.append(word2idx[word])\n",
    "                except:\n",
    "                    new_doc.append(0)\n",
    "            new_corpus.append(new_doc)\n",
    "        return np.array(new_corpus)\n",
    "\n",
    "    PADDING_LENGTH = 256\n",
    "    X = text_to_index(rnn_feature)\n",
    "    X = pad_sequences(X, maxlen=PADDING_LENGTH)\n",
    "    #print(\"Shape:\", X.shape)\n",
    "    #print(\"Sample:\", X[100])\n",
    "    \n",
    "    print(\"Save data...\\n\")\n",
    "    # 存檔\n",
    "    save = np.array(X)\n",
    "    np.save(\"./feature/rnn_feature\" + str(num) + \".npy\", save)\n",
    "    #np.save(\"./feature/rnn_test_feature.npy\", save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858.5670750141144\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "w2v_model = Word2Vec(rnn_feature,min_count = 4, size=200, sg = 1)\n",
    "w2v_model.save(\"large_w2v.model\")\n",
    "end = time.time()\n",
    "spend = end - start\n",
    "print(spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡負責數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "keys_strings = ['avlength', 'numstrings', 'registry', 'urls', 'MZ', 'printables', 'entropy', 'paths', 'printabledist']\n",
    "keys_general = ['exports', 'has_resources', 'imports', 'symbols', 'has_signature', 'has_relocations', 'has_debug', 'vsize', 'size', 'has_tls']\n",
    "\n",
    "for num in range(7):\n",
    "    \n",
    "    print(num)\n",
    "    \n",
    "    path = \"./ember/train_features_\"+ str(num) +\".jsonl\"\n",
    "    \n",
    "    if num == 6:\n",
    "        path = \"./ember/test_features.jsonl\"\n",
    "        \n",
    "    origin_feature = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            origin_feature.append(obj)\n",
    "    n = len(origin_feature)\n",
    "    print(n)\n",
    "    output = []\n",
    "    for obj in range(n):\n",
    "        feature_num = []\n",
    "        for i in range(4):\n",
    "            feature_num.append([])\n",
    "        for j in keys_strings:\n",
    "            if j != 'printabledist':\n",
    "                feature_num[0].append(origin_feature[obj]['strings'][j])\n",
    "            else:\n",
    "                for k in range(96):\n",
    "                    feature_num[0].append(origin_feature[obj]['strings'][j][k])\n",
    "        for i in range(256):\n",
    "            feature_num[1].append(origin_feature[obj]['histogram'][i])\n",
    "        for i in range(256):\n",
    "            feature_num[2].append(origin_feature[obj]['byteentropy'][i])\n",
    "        for i in keys_general:\n",
    "            feature_num[3].append(origin_feature[obj]['general'][i])\n",
    "        for i in range(256 - 104):\n",
    "            feature_num[0].append(0)\n",
    "        for i in range(256 - 10):\n",
    "            feature_num[3].append(0)\n",
    "        output.append(feature_num)\n",
    "    tmp = np.array(output)\n",
    "    np.save(\"./feature/num_feature\"+str(num)+\".npy\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡把string還有number串接起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "Import npy...\n",
      "\n",
      "Deal Data...\n",
      "\n",
      "Save Data...\n",
      "\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for num in range(7):\n",
    "    if num != 6:\n",
    "        print(\"Import npy...\\n\")\n",
    "        tmp = np.load(\"./feature/num_feature\"+str(num)+\".npy\")\n",
    "        save = np.load(\"./feature/rnn_feature\"+str(num)+\".npy\")\n",
    "        save_n = save.shape[0]\n",
    "        print(\"Deal Data...\\n\")\n",
    "        save = save.reshape(save_n, 1, 256)\n",
    "        all_feature = np.hstack((tmp,save))\n",
    "        print(\"Save Data...\\n\")\n",
    "        np.save(\"./feature/all_features\"+str(num)+\".npy\",all_feature)\n",
    "    else:\n",
    "        print(\"test\")\n",
    "        # test data\n",
    "        tmp = np.load(\"./feature/num_feature6.npy\")\n",
    "        save = np.load(\"./feature/rnn_feature6.npy\")\n",
    "        save_n = save.shape[0]\n",
    "        save = save.reshape(save_n, 1, 256)\n",
    "        all_feature = np.hstack((tmp,save))\n",
    "        np.save(\"./feature/all_test.npy\",all_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
